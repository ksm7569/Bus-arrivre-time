{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6c5b9a-5ede-4ee2-a80d-118c82f4b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6960\\386267076.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"now_arrive_time\"] = pd.to_datetime(df[\"now_arrive_time\"])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: 06시, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateParseError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]).dt.hour\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 도착 시간 차이 계산\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mnow_arrive_time\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnow_arrive_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mnext_arrive_time\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mnext_arrive_time\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     12\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtime_diff\u001b[39m\u001b[33m\"\u001b[39m] = (df[\u001b[33m\"\u001b[39m\u001b[33mnext_arrive_time\u001b[39m\u001b[33m\"\u001b[39m] - df[\u001b[33m\"\u001b[39m\u001b[33mnow_arrive_time\u001b[39m\u001b[33m\"\u001b[39m]).dt.total_seconds()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\pytorch_env01\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1068\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1066\u001b[39m             result = arg.tz_localize(\u001b[33m\"\u001b[39m\u001b[33mutc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     cache_array = \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array.empty:\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\pytorch_env01\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:249\u001b[39m, in \u001b[36m_maybe_cache\u001b[39m\u001b[34m(arg, format, cache, convert_listlike)\u001b[39m\n\u001b[32m    247\u001b[39m unique_dates = unique(arg)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) < \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     cache_dates = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\pytorch_env01\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:437\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    449\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\envs\\pytorch_env01\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2415\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2412\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2413\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2426\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:412\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:553\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/conversion.pyx:641\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:336\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:666\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDateParseError\u001b[39m: Unknown datetime string format, unable to parse: 06시, at position 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"C:\\\\ai_project01\\\\버스도착시간\\\\jeju_bus.csv\")\n",
    "\n",
    "# date 컬럼에서 시(hour) 뽑기\n",
    "df[\"hour\"] = pd.to_datetime(df[\"date\"]).dt.hour\n",
    "\n",
    "# 도착 시간 차이 계산\n",
    "df[\"now_arrive_time\"] = pd.to_datetime(df[\"now_arrive_time\"])\n",
    "df[\"next_arrive_time\"] = pd.to_datetime(df[\"next_arrive_time\"])\n",
    "df[\"time_diff\"] = (df[\"next_arrive_time\"] - df[\"now_arrive_time\"]).dt.total_seconds()\n",
    "\n",
    "# 속도 계산 (m/s)\n",
    "df[\"speed\"] = df[\"distance\"] / df[\"time_diff\"].replace(0, pd.NA)\n",
    "\n",
    "# 모델 입력에 필요한 X 구성\n",
    "X = df[[\n",
    "    \"now_latitude\",\"now_longitude\",\"distance\",\n",
    "    \"next_latitude\",\"next_longitude\",\"hour\",\"speed\"\n",
    "]]\n",
    "\n",
    "df\n",
    "\n",
    "# 인공지능 모델의 입력으로 사용할 데이터를 X에 저장합니다.\n",
    "# 이 데이터는 '현재 위도(now_latitude)', '현재 경도(now_longitude)', '거리(distance)',\n",
    "# '다음 정류장 위도(next_latitude)', '다음 정류장 경도(next_longitude)', '시간(hour)', '속도(speed)'\n",
    "# 를 포함합니다.\n",
    "\n",
    "X = df[[\n",
    "    \"now_latitude\", 'now_longitude', 'distance',\n",
    "    'next_latitude', 'next_longitude', 'hour', 'speed'\n",
    "]]\n",
    "\n",
    "# X 데이터를 화면에 출력하여 제대로 선택되었는지 확인합니다.\n",
    "X\n",
    "\n",
    "# X 데이터의 각 열(변수)별 평균을 계산하여 X_mean에 저장합니다.\n",
    "# 평균(mean)은 데이터를 대표하는 하나의 값으로, 모든 값을 더한 후 개수로 나눈 값입니다.\n",
    "X_mean = X.mean()\n",
    "\n",
    "# 계산된 X 데이터 평균을 화면에 출력합니다.\n",
    "X_mean\n",
    "\n",
    "# X 데이터의 각 열별 표준편차를 계산하여 X_std에 저장합니다.\n",
    "# 표준편차(std)는 데이터가 평균을 기준으로 얼마나 퍼져있는지를 나타내는 값입니다.\n",
    "X_std = X.std()\n",
    "\n",
    "# 계산된 X 데이터 표준편차를 화면에 출력합니다.\n",
    "X_std\n",
    "\n",
    "# 데이터의 표준화(Standardization)를 진행합니다.\n",
    "# 표준화는 데이터의 평균이 0, 표준편차가 1이 되도록 만들어주는 과정입니다.\n",
    "# 표준화 수식: (원래 값 - 평균) / 표준편차\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# 표준화된 X 데이터를 화면에 출력하여 변환된 결과를 확인합니다.\n",
    "X_scaled\n",
    "\n",
    "# 인공지능 모델이 예측할 목표값(정답 데이터)을 y에 저장합니다.\n",
    "# 예측하려는 값은 '다음 정류장까지 걸리는 시간(next_arrive_time)'입니다.\n",
    "y = df[['next_arrive_time']]\n",
    "\n",
    "# y 데이터를 화면에 출력하여 목표값이 제대로 선택되었는지 확인합니다.\n",
    "y\n",
    "\n",
    "# y 데이터프레임을 \"next_arrive_time\"(버스가 다음 정류장에 도착하는 데 걸리는 시간) 칼럼의 값을 기준으로 정렬하는 코드입니다.\n",
    "# sort_values() 함수는 특정 열의 값을 기준으로 데이터프레임을 정렬할 때 사용하는 판다스(pandas)의 함수입니다.\n",
    "# 매개변수(by): 어떤 열(column)을 기준으로 정렬할지 정해줍니다. 여기서는 'next_arrive_time' 칼럼을 기준으로 정렬합니다.\n",
    "# 매개변수(ascending): 정렬의 방향을 정합니다.\n",
    "#   - ascending=False로 설정하면 내림차순(큰 값 -> 작은 값)으로 정렬합니다.\n",
    "#   - 예를 들어 도착시간이 긴 데이터부터 짧은 데이터 순으로 정렬됩니다.\n",
    "# 이 작업을 통해 시간이 가장 오래 걸리는 데이터부터 순서대로 쉽게 확인할 수 있습니다.\n",
    "y.sort_values(by=\"next_arrive_time\", ascending=False)\n",
    "\n",
    "# y 데이터의 평균을 계산하여 y_mean에 저장합니다.\n",
    "# 이는 목표 데이터(정답값)의 평균을 의미합니다.\n",
    "y_mean = y.mean()\n",
    "\n",
    "# 계산된 y 데이터의 평균을 화면에 출력합니다.\n",
    "y_mean\n",
    "\n",
    "# y 데이터의 표준편차를 계산하여 y_std에 저장합니다.\n",
    "y_std = y.std()\n",
    "\n",
    "# 계산된 y 데이터의 표준편차를 화면에 출력합니다.\n",
    "y_std\n",
    "\n",
    "# y 데이터도 표준화를 진행합니다.\n",
    "# 목표 데이터도 평균이 0, 표준편차가 1이 되도록 만듭니다.\n",
    "y_scaled = (y - y_mean) / y_std\n",
    "\n",
    "# 표준화된 y 데이터를 화면에 출력하여 확인합니다.\n",
    "y_scaled\n",
    "\n",
    "# 데이터를 '학습 데이터(X_train, y_train)'와 '테스트 데이터(X_test, y_test)'로 나누는 과정입니다.\n",
    "# 인공지능 모델은 전체 데이터를 사용해 학습을 하는 것이 아니라,\n",
    "# 일부는 모델을 훈련(학습)하는 데 사용하고 나머지 일부는 모델이 얼마나 잘 학습되었는지 평가하는 데 사용합니다.\n",
    "\n",
    "# 사용하는 함수는 train_test_split 입니다.\n",
    "# train_test_split은 데이터를 두 부분으로 나눠주는 기능을 하는 scikit-learn의 함수입니다.\n",
    "# 주로 데이터를 분할하여 인공지능 모델을 만들 때 사용됩니다.\n",
    "\n",
    "# train_test_split 함수의 매개변수(설정값)에 대해 설명합니다.\n",
    "# 매개변수(parameter)를 잘못 넣을 경우 동일한 코드라도, 이럴 때마다 학습의 동작 방식도 달라집니다.\n",
    "# - 첫 번째 매개변수 (X_scaled): 입력 데이터입니다. 표준화된 입력값(X_scaled)을 사용합니다.\n",
    "# - 두 번째 매개변수 (y_scaled): 목표 데이터입니다. 표준화된 목표값(y_scaled)을 사용합니다.\n",
    "# - test_size=0.2: 테스트 데이터의 크기를 의미합니다.\n",
    "#   → 0.2는 전체 데이터의 20%를 테스트 데이터로 사용하고, 나머지 80%를 학습 데이터로 사용하겠다는 뜻입니다.\n",
    "# - random_state=42: 데이터를 나눌 때 사용하는 난수(무작위 수) 생성기의 시드(seed)입니다.\n",
    "#   → 매번 같은 데이터로 생성하기 위해 넣는 것으로, 결과의 재현 가능성(매번 같은 결과를 얻을 수 있도록)을 위해 사용됩니다.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# X_train은 인공지능 모델을 학습시킬 때 입력으로 사용되는 데이터입니다.\n",
    "# 전체 데이터의 80%에 해당하며, 모델이 이 데이터를 통해 학습합니다.\n",
    "X_train\n",
    "\n",
    "# X_test는 인공지능 모델의 성능을 평가할 때 사용하는 입력 데이터입니다.\n",
    "# 모델이 처음 보는 새로운 데이터이며, 전체 데이터의 20%에 해당합니다.\n",
    "X_test\n",
    "\n",
    "# y_train은 인공지능 모델을 학습시킬 때 사용하는 정답(목표값) 데이터입니다.\n",
    "# X_train과 같은 비율로 나누어지며, 입력 X_train의 정답이 됩니다.\n",
    "y_train\n",
    "\n",
    "# y_test는 인공지능 모델의 성능을 평가할 때 사용되는 실제 정답 데이터입니다.\n",
    "# 모델이 X_test 입력 데이터를 통해 예측한 값과 비교하여 얼마나 정확한지 평가합니다.\n",
    "y_test\n",
    "\n",
    "# 파이토치(PyTorch)의 텐서(tensor)로 데이터를 변환하는 과정입니다.\n",
    "# 텐서(tensor)는 파이토치에서 데이터를 다룰 때 사용하는 특별한 형태의 데이터입니다.\n",
    "# 쉽게 말해, 텐서는 파이토치에서 숫자를 효율적으로 계산하기 위해 데이터 저장소입니다.\n",
    "# 이 과정이 필요한 이유는 인공지능 모델이 텐서 형태로 입력 데이터를 받기 때문입니다.\n",
    "\n",
    "# torch.tensor() 함수 설명:\n",
    "# torch.tensor()는 리스트, 배열 또는 판다스 데이터 등을 텐서로 바꾸는 함수입니다.\n",
    "# 이 함수는 인공지능 모델 학습을 위해 꼭 필요합니다.\n",
    "\n",
    "# torch.tensor()의 매개변수(parameter):\n",
    "# - 첫 번째 매개변수(X_train.values 또는 y_train.values): 변환할 데이터입니다.\n",
    "#   판다스(pandas)의 데이터프레임은 .values를 사용하면 넘파이(numpy) 배열 형태로 바꿔줄 수 있습니다.\n",
    "# - dtype=torch.float32: 텐서 내부의 데이터 형식을 지정합니다.\n",
    "#   torch.float32는 실수(소수점을 포함한 숫자)를 저장할 때 사용하는 데이터 타입입니다.\n",
    "\n",
    "# 학습 데이터 X_train(모델 입력 데이터)을 텐서로 변환합니다.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "\n",
    "X_train_tensor\n",
    "\n",
    "# 테스트 데이터 X_test(모델 평가에 사용될 입력 데이터)를 텐서로 변환합니다.\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor\n",
    "\n",
    "# 학습 데이터 y_train(정답 데이터)을 텐서로 변환합니다.\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor\n",
    "\n",
    "# 테스트 데이터 y_test(모델 평가에 사용될 정답 데이터)를 텐서로 변환합니다.\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "y_test_tensor\n",
    "\n",
    "# input_dim이라는 변수에 입력 데이터(X_train)의 열(column) 개수를 저장합니다.\n",
    "# 인공지능 모델이 입력 데이터의 크기를 미리 알아야 학습을 할 수 있기 때문입니다.\n",
    "# X_train_tensor.shape[1]에서 shape는 데이터의 모양을 알려주는 속성입니다.\n",
    "# shape[1]는 데이터의 열(column)의 개수를 의미합니다.\n",
    "# 예를 들어 데이터가 100개의 행(row)과 7개의 열(column)로 구성되었다면, shape는 (100, 7)이 되고 shape[1]은 7이 됩니다.\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "\n",
    "# input_dim 값을 출력해서 입력 데이터의 크기(특성 개수)를 확인합니다.\n",
    "input_dim\n",
    "\n",
    "# 선형 회귀 모델을 생성하는 코드입니다.\n",
    "# 선형 회귀 모델이란 입력 데이터를 이용해서 연속적인 숫자(예를 들어, 도착 시간)를 예측하는 가장 기본적인 인공지능 모델입니다.\n",
    "# 입력값과 출력값 사이의 관계를 가장 잘 설명하는 직선을 찾는 방식입니다.\n",
    "\n",
    "# torch.nn.Linear 함수 설명:\n",
    "# - PyTorch에서 제공하는 간단한 선형 회귀 모델입니다.\n",
    "# - 입력 데이터를 받아서 하나의 결과값(출력)을 계산하는 역할을 합니다.\n",
    "# - 내부적으로 출력값 = (입력값 × 가중치) + 편향(절편)이라는 수식을 사용합니다.\n",
    "\n",
    "# 매개변수(parameter) 설명:\n",
    "# - input_dim: 입력 데이터의 특성(변수)의 개수입니다. 예를 들어 7개의 입력값을 사용하면 7이 됩니다.\n",
    "# - 1: 모델에 예측할 출력 값의 개수입니다. 여기서는 버스의 도착 시간 하나만을 예측하므로 1입니다.\n",
    "model = torch.nn.Linear(input_dim, 1)\n",
    "\n",
    "# 생성된 모델을 화면에 출력하여 모델의 구조를 확인합니다.\n",
    "model\n",
    "\n",
    "# 손실 함수(loss function)를 설정하는 코드입니다.\n",
    "# 손실 함수는 인공지능 모델이 예측한 값과 실제 값 사이의 차이를 계산하는 함수입니다.\n",
    "# 이 차이(오차)를 최소화하는 것이 인공지능 학습의 목표입니다.\n",
    "\n",
    "# torch.nn.MSELoss 함수 설명:\n",
    "# - MSELoss는 Mean Squared Error(평균 제곱 오차)를 계산하는 함수입니다.\n",
    "# - 예측값과 실제값의 차이를 제곱하여 평균을 내는 방식으로 오차를 계산합니다.\n",
    "# - 값이 작을수록 예측이 정확하다는 의미입니다.\n",
    "\n",
    "# 매개변수(parameter) 설명:\n",
    "# - MSELoss 함수는 별도의 매개변수를 설정하지 않아도 기본 설정으로 작동합니다.\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 손실 함수 구조를 출력하여 확인합니다.\n",
    "criterion\n",
    "\n",
    "# 손실 함수(loss function)를 설정하는 코드입니다.\n",
    "# 손실 함수는 인공지능 모델이 예측한 값과 실제 값 사이의 차이를 계산하는 함수입니다.\n",
    "# 이 차이(오차)를 최소화하는 것이 인공지능 학습의 목표입니다.\n",
    "\n",
    "# torch.nn.MSELoss 함수 설명:\n",
    "# - MSELoss는 Mean Squared Error(평균 제곱 오차)를 계산하는 함수입니다.\n",
    "# - 예측값과 실제값의 차이를 제곱하여 평균을 내는 방식으로 오차를 계산합니다.\n",
    "# - 값이 작을수록 예측이 정확하다는 의미입니다.\n",
    "\n",
    "# 매개변수(parameter) 설명:\n",
    "# - MSELoss 함수는 별도의 매개변수를 설정하지 않아도 기본 설정으로 작동합니다.\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 손실 함수 구조를 출력하여 확인합니다.\n",
    "criterion\n",
    "\n",
    "# 옵티마이저(optimizer)를 설정하는 코드입니다.\n",
    "# 옵티마이저는 손실 함수를 최소화하기 위해 모델의 가중치(weight)와 편향(bias)을 자동으로 조정하는 역할을 합니다.\n",
    "# 사람이 직접 값을 바꾸는 것이 아니라, 인공지능이 스스로 학습을 하여 최적의 값을 찾아가도록 돕습니다.\n",
    "\n",
    "# torch.optim.Adam 함수 설명:\n",
    "# - Adam(Adaptive Moment Estimation)은 최신 옵티마이저 중 하나로, 매우 효율적이고 빠르게 학습하는 데 도움을 줍니다.\n",
    "# - 자동으로 가중치와 편향의 값을 최적화하여 손실 함수를 최소화합니다.\n",
    "\n",
    "# 매개변수(parameter) 설명:\n",
    "# - model.parameters(): 모델이 가지고 있는 모든 학습 가능한 파라미터(가중치, 편향)를 의미합니다.\n",
    "# - lr=0.001: 학습률(Learning rate)을 설정하는 부분입니다.\n",
    "#   학습률은 가중치와 편향을 얼마나 크게 변경할지를 결정하는 값으로, 너무 크면 학습이 불안정해지고 너무 작으면 학습이 느려집니다.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 옵티마이저 설정을 화면에 출력하여 확인합니다.\n",
    "optimizer\n",
    "\n",
    "# 📖 인공지능 모델을 학습(훈련)할 횟수를 정하는 코드입니다.\n",
    "# 'epoch'는 전체 데이터를 한 번씩 모두 사용하여 학습하는 단위를 의미합니다.\n",
    "# 이 횟수가 많을수록 모델은 데이터를 여러 번 보고 더욱 정확하게 예측할 수 있게 됩니다.\n",
    "# 일반적으로 100~1000회 사이로 설정하는 경우가 많습니다.\n",
    "epochs = 1000\n",
    "\n",
    "# 반복문을 이용하여 설정한 횟수만큼 모델 학습을 진행합니다.\n",
    "# for epoch in range(epochs)는 0부터 epochs-1까지의 숫자를 하나씩 사용하며 반복합니다.\n",
    "# 즉, 1000회(epoch 0부터 epoch 999까지) 모델을 학습하는 과정입니다.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 모델이 현재 가지고 있는 가중치(weight)와 편향(bias)을 사용하여 입력 데이터(X_train_tensor)를 기반으로 예측을 합니다.\n",
    "    # model(X_train_tensor)은 학습 데이터를 입력으로 받아서, 모델이 학습한 규칙을 사용해 출력값을 예측하는 기능입니다.\n",
    "    predictions = model(X_train_tensor)\n",
    "\n",
    "    # 예측값(predictions)과 실제값(y_train_tensor)의 차이를 손실(Loss)로 계산합니다.\n",
    "    # criterion(predictions, y_train_tensor)는 미리 설정한 손실 함수(MSELoss)를 사용하여 손실 값을 계산합니다.\n",
    "    # MSELoss(평균 제곱 오차)는 예측값과 실제값의 차이를 제곱한 뒤 평균을 계산하는 방식으로 손실을 구합니다.\n",
    "    # 손실값이 작을수록 모델이 잘 예측하고 있다는 것을 의미합니다.\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "\n",
    "    # 모델이 이전에 계산한 기울기(gradient)를 초기화합니다.\n",
    "    # optimizer.zero_grad()는 지난 반복(epoch)에서 계산된 기울기를 모두 0으로 초기화하는 함수입니다.\n",
    "    # 파이토치는 자동으로 기울기를 누적해 나가기 때문에, 새로운 epoch마다 꼭 초기화를 해주어야 정확한 기울기를 계산할 수 있습니다.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 현재 손실(loss)에 대한 기울기를 계산합니다.\n",
    "    # loss.backward()는 오차를 기준으로 모델 파라미터(weight와 bias)에 대한 기울기를 자동으로 계산하는 함수입니다.\n",
    "    # 이 과정은 '역전파(Backpropagation)'라고 불리며, 인공지능 학습에서 매우 핵심적인 단계입니다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 옵티마이저를 이용해 모델의 파라미터를 업데이트합니다.\n",
    "    # optimizer.step()은 위에서 계산된 기울기(gradient)를 이용해 모델의 가중치(weight)와 편향(bias)를 조금씩 조정하는 함수입니다.\n",
    "    # 조정되는 폭은 학습률(Learning rate) 값에 따라 결정되며, 이 과정을 통해 손실을 줄이고 예측 정확도를 높여 나갑니다.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 현재 epoch에서의 손실값을 출력합니다.\n",
    "    # loss.item()은 텐서 형태로 저장된 손실 값을 일반 숫자(float)로 변환하는 함수입니다.\n",
    "    # f-string을 이용해 Epoch 번호와 손실값을 보기 좋게 출력해줍니다.\n",
    "    print(f\"Epoch [{epoch}] cost: {loss.item()}\")\n",
    "\n",
    "# 테스트 데이터로 인공지능 모델의 성능을 확인하는 과정입니다.\n",
    "\n",
    "# torch.no_grad() 설명:\n",
    "# 이 코드는 '기울기 계산을 하지 않겠다'는 뜻입니다.\n",
    "# 기울기는 모델 학습에 필요할 때만 계산하며, 모델 평가 시에는 불필요한 연산을 줄이기 위해서 기울기를 계산하지 않습니다.\n",
    "with torch.no_grad():\n",
    "\n",
    "    # 모델이 학습한 규칙을 이용해 테스트 데이터(X_test_tensor)를 가지고 예측을 수행합니다.\n",
    "    # model() 함수는 인공지능 모델이 가진 규칙(훈련된 가중치와 편향)을 이용해 입력 데이터를 받아 예측값을 출력합니다.\n",
    "    # 매개변수(parameter): X_test_tensor (표준화된 테스트 입력 데이터입니다)\n",
    "    predicted = model(X_test_tensor)\n",
    "\n",
    "    # 예측값과 실제값의 차이를 손실(loss)로 계산합니다.\n",
    "    # criterion()은 미리 정의한 손실 함수로, 여기서는 평균 제곱 오차(MSELoss)를 사용합니다.\n",
    "\n",
    "    # 매개변수(parameter):\n",
    "    # - predicted: 모델이 예측한 값입니다.\n",
    "    # - y_test_tensor: 실제 값(정답 데이터)입니다.\n",
    "    test_loss_scaled = criterion(predicted, y_test_tensor)\n",
    "\n",
    "    # 계산된 손실(Loss)을 출력합니다. 이 손실 값은 표준화된 상태의 값입니다.\n",
    "    # .item() 함수는 PyTorch 텐서(tensor)를 일반 숫자형(float형)으로 변환하는 기능입니다.\n",
    "    print(f'Test Loss scaled: {test_loss_scaled.item():.4f}')\n",
    "    \n",
    "    # 손실(Loss)을 원래의 값으로 복원합니다.\n",
    "    # 손실 값이 표준화되어 있어 이해하기 어려우므로 원래의 값으로 되돌려서 해석하기 쉽게 만듭니다.\n",
    "    # 실제 손실 = (표준화된 손실 × 목표값의 표준편차) + 목표값의 평균\n",
    "    real_test_loss = (test_loss_scaled.item() * y_std) + y_mean\n",
    "    \n",
    "    # 복원된 실제 손실 값을 출력합니다. 이 값은 모델의 성능을 직관적으로 이해할 수 있는 값입니다.\n",
    "    print(f'Test Loss: {real_test_loss.item():.4f}')\n",
    "\n",
    "# 새로운 데이터를 준비하여 인공지능 모델이 버스 도착 시간을 예측할 수 있도록 합니다.\n",
    "# 이 데이터는 인공지능 모델이 학습한 데이터 형식과 똑같아야 합니다.\n",
    "\n",
    "# pd.DataFrame() 함수는 표 형식으로 데이터를 만드는 pandas 라이브러리의 함수입니다.\n",
    "# 여기서는 인공지능 모델에 예측에 사용할 입력 데이터로 사용됩니다.\n",
    "# 매개변수(parameter)는 딕셔너리(dictionary) 형태로 데이터를 입력합니다.\n",
    "# 딕셔너리는 '{열 이름: [데이터]}' 형식으로 이루어집니다.\n",
    "new_data = pd.DataFrame({\n",
    "    'now_latitude': [33.456267],     # 현재 정류장의 위도 값입니다. 남국사 정류장 입니다\n",
    "    'now_longitude': [126.551575],   # 현재 정류장의 경도 값입니다. 남국사 정류장 입니다\n",
    "    'distance': [266.0],             # 다음 정류장까지의 거리(미터)입니다.\n",
    "    'next_latitude': [33.457724],    # 다음 정류장의 위도 값입니다. 제대마을 정류장 입니다\n",
    "    'next_longitude': [126.554014],  # 다음 정류장의 경도 값입니다. 제대마을 정류장 입니다\n",
    "    'hour': [6],                     # 현재 시간(24시 단위)입니다.\n",
    "    'speed': [11.08],                # 버스 속도(미터/초)입니다.\n",
    "})\n",
    "\n",
    "# 새로운 데이터 표준화\n",
    "# 학습 때와 같은 방법으로 데이터를 표준화해야 합니다.\n",
    "# 동일한 입력값이라도 입력 값의 평균 및 표준편차를 이용해 같은 방식으로 표준화해야 합니다.\n",
    "new_data_scaled = (new_data - X_mean) / X_std\n",
    "\n",
    "# 데이터를 파이토치(PyTorch)의 텐서(Tensor)로 변환합니다.\n",
    "# 파이토치 모델에서는 사용되는 입력도 텐서 형태여야 하므로, 모델에 입력하려면 데이터를 텐서로 변환해야 합니다.\n",
    "# torch.tensor()는 데이터를 텐서로 바꾸는 함수이며, 이때는 주로 float32 타입을 사용합니다.\n",
    "# dtype=torch.float32 : 텐서 데이터의 타입을 실수(float32)로 설정합니다.\n",
    "new_data_tensor = torch.tensor(new_data_scaled.values, dtype=torch.float32)\n",
    "\n",
    "# 모델을 평가 모드(eval)로 전환합니다.\n",
    "# 학습 모드(train)과 평가 모드는 내부적으로 동작 방식이 다릅니다.\n",
    "# - 학습 모드: 학습에 필요한 기능을 활성화\n",
    "# - 평가 모드: Dropout이나 BatchNorm 같은 학습 전용 함수는 모두 off.\n",
    "model.eval()\n",
    "\n",
    "# torch.no_grad()를 사용하여 기울기를 계산하지 않고 순수 예측만 진행합니다.\n",
    "with torch.no_grad():\n",
    "    # 준비한 새로운 입력 데이터를 모델에 넣고 예측 수행\n",
    "    predicted_scaled = model(new_data_tensor)\n",
    "\n",
    "# 예측한 값을 원래 값으로 복원합니다.\n",
    "# 표준화된 예측값이라 사람이 해석하기 어렵기 때문에, 원래의 값(초 단위) 정도로 되돌려줍니다.\n",
    "# 예측 값 = 표준화된 예측값 × 목표값의 표준편차 + 목표값의 평균 방식으로 계산합니다.\n",
    "predicted_real = (predicted_scaled.item() * y_std) + y_mean\n",
    "\n",
    "# 예측된 버스 도착 시간을 화면에 출력합니다.\n",
    "# .item()은 텐서 값을 일반 숫자(float)로 변환합니다.\n",
    "print(f\"\\n✅ 새 입력 데이터에 예측된 버스 도착시간: {predicted_real.item():.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ab370-1d26-471a-84c3-094951993eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943fd9e-622d-4474-85d2-91607686f82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
